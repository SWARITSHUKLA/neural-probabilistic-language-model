{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "876667e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets build the model :)\n"
     ]
    }
   ],
   "source": [
    "print('Lets build the model :)')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#In this modeling approach we are taking 3 previous and trying to predict the 4th word in the sequance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "be210e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "141884cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "28dc6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the mappings of characters to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4ff63d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the dataset\n",
    "block_size = 3 #context length how many characters do we take to predict the next one\n",
    "X,Y = [],[]\n",
    "for w in words[:5]:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] #crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ce9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "483c71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(219384384)\n",
    "#creating a lookup table for the character embeddings\n",
    "C = torch.randn((27,2), generator = g)\n",
    "# Constructiong the hidden layer\n",
    "#The number of inputs to this layer is going to be 3 * 2  because we hae to dimensional embeddings and we have 3 of them\n",
    "# and its up to us to decide how many neurons we want inside the layer here we are going with 100 of them\n",
    "W1 = torch.randn(6,100, generator = g )\n",
    "b1 = torch.randn(100, generator = g) \n",
    "W2 = torch.randn(100, 27, generator = g) #our second layer will take 100 inputs and \n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C,W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "98c3cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #tells us number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a18a2d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.5619)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] #the shape is (32,3,2)\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) #we are using tanh activation fn so the numbers in h will be -1 and 1 \n",
    "# We pass -1 to emb.view(), becuase we want pytorch to guess what what will be the right number for rows given we have already told it the number of columns are 6\n",
    "# here you can also use emb.reshape(32,6) the difference is emb.view() will not use extra space it ensure that the emb tensor and and the new tensor that we create will use the same data so no memory wastag\n",
    "logits = h @ W2 + b2 #the shape is 32,27\n",
    "counts = logits.exp() \n",
    "prob = counts/ counts.sum(dim = 1, keepdim= True)\n",
    "# implementing the loss function\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0858cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
