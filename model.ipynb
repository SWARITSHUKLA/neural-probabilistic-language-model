{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "876667e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets build the model :)\n"
     ]
    }
   ],
   "source": [
    "print('Lets build the model :)')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#In this modeling approach we are taking 3 previous and trying to predict the 4th word in the sequance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "be210e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "141884cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "28dc6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the mappings of characters to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4ff63d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the dataset\n",
    "block_size = 3 #context length how many characters do we take to predict the next one\n",
    "X,Y = [],[]\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] #crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "483c71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(219384384)\n",
    "#creating a lookup table for the character embeddings\n",
    "C = torch.randn((27,2), generator = g)\n",
    "# Constructiong the hidden layer\n",
    "#The number of inputs to this layer is going to be 3 * 2  because we hae to dimensional embeddings and we have 3 of them\n",
    "# and its up to us to decide how many neurons we want inside the layer here we are going with 100 of them\n",
    "W1 = torch.randn(6,100, generator = g )\n",
    "b1 = torch.randn(100, generator = g) \n",
    "W2 = torch.randn(100, 27, generator = g) #our second layer will take 100 inputs and \n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C,W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "98c3cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #tells us number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b3cc23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "#we do this because p.requires_grad is false by default but insted of treating these tensors as constant we want pytorch to treat them as variable, Variable which requires to have gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a18a2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 18.32672882080078 for iteration= 0\n",
      "loss= 16.474193572998047 for iteration= 1\n",
      "loss= 15.277494430541992 for iteration= 2\n",
      "loss= 14.292076110839844 for iteration= 3\n",
      "loss= 13.503080368041992 for iteration= 4\n",
      "loss= 12.799945831298828 for iteration= 5\n",
      "loss= 12.176278114318848 for iteration= 6\n",
      "loss= 11.639505386352539 for iteration= 7\n",
      "loss= 11.172645568847656 for iteration= 8\n",
      "loss= 10.7584867477417 for iteration= 9\n",
      "loss= 10.384634017944336 for iteration= 10\n",
      "loss= 10.042563438415527 for iteration= 11\n",
      "loss= 9.726486206054688 for iteration= 12\n",
      "loss= 9.43183708190918 for iteration= 13\n",
      "loss= 9.155553817749023 for iteration= 14\n",
      "loss= 8.895856857299805 for iteration= 15\n",
      "loss= 8.6515474319458 for iteration= 16\n",
      "loss= 8.421377182006836 for iteration= 17\n",
      "loss= 8.203655242919922 for iteration= 18\n",
      "loss= 7.996348857879639 for iteration= 19\n",
      "loss= 7.7974138259887695 for iteration= 20\n",
      "loss= 7.6050639152526855 for iteration= 21\n",
      "loss= 7.417822360992432 for iteration= 22\n",
      "loss= 7.234484672546387 for iteration= 23\n",
      "loss= 7.0541768074035645 for iteration= 24\n",
      "loss= 6.876632213592529 for iteration= 25\n",
      "loss= 6.702510833740234 for iteration= 26\n",
      "loss= 6.533232688903809 for iteration= 27\n",
      "loss= 6.370121479034424 for iteration= 28\n",
      "loss= 6.213822841644287 for iteration= 29\n",
      "loss= 6.064596652984619 for iteration= 30\n",
      "loss= 5.9226579666137695 for iteration= 31\n",
      "loss= 5.788239002227783 for iteration= 32\n",
      "loss= 5.661437511444092 for iteration= 33\n",
      "loss= 5.542148590087891 for iteration= 34\n",
      "loss= 5.430112838745117 for iteration= 35\n",
      "loss= 5.324975967407227 for iteration= 36\n",
      "loss= 5.226319789886475 for iteration= 37\n",
      "loss= 5.133713722229004 for iteration= 38\n",
      "loss= 5.046749114990234 for iteration= 39\n",
      "loss= 4.965056896209717 for iteration= 40\n",
      "loss= 4.888305187225342 for iteration= 41\n",
      "loss= 4.816180229187012 for iteration= 42\n",
      "loss= 4.748378276824951 for iteration= 43\n",
      "loss= 4.684598922729492 for iteration= 44\n",
      "loss= 4.624558925628662 for iteration= 45\n",
      "loss= 4.567988395690918 for iteration= 46\n",
      "loss= 4.5146484375 for iteration= 47\n",
      "loss= 4.464315414428711 for iteration= 48\n",
      "loss= 4.4167866706848145 for iteration= 49\n",
      "loss= 4.371869087219238 for iteration= 50\n",
      "loss= 4.329377174377441 for iteration= 51\n",
      "loss= 4.289133071899414 for iteration= 52\n",
      "loss= 4.250962257385254 for iteration= 53\n",
      "loss= 4.214698314666748 for iteration= 54\n",
      "loss= 4.180182456970215 for iteration= 55\n",
      "loss= 4.147267818450928 for iteration= 56\n",
      "loss= 4.115818023681641 for iteration= 57\n",
      "loss= 4.0857086181640625 for iteration= 58\n",
      "loss= 4.05682897567749 for iteration= 59\n",
      "loss= 4.02908182144165 for iteration= 60\n",
      "loss= 4.002378463745117 for iteration= 61\n",
      "loss= 3.976642608642578 for iteration= 62\n",
      "loss= 3.951805353164673 for iteration= 63\n",
      "loss= 3.9278061389923096 for iteration= 64\n",
      "loss= 3.9045891761779785 for iteration= 65\n",
      "loss= 3.8821051120758057 for iteration= 66\n",
      "loss= 3.86030912399292 for iteration= 67\n",
      "loss= 3.839158058166504 for iteration= 68\n",
      "loss= 3.8186120986938477 for iteration= 69\n",
      "loss= 3.798633575439453 for iteration= 70\n",
      "loss= 3.779186964035034 for iteration= 71\n",
      "loss= 3.7602388858795166 for iteration= 72\n",
      "loss= 3.74175763130188 for iteration= 73\n",
      "loss= 3.723712921142578 for iteration= 74\n",
      "loss= 3.7060770988464355 for iteration= 75\n",
      "loss= 3.6888232231140137 for iteration= 76\n",
      "loss= 3.6719279289245605 for iteration= 77\n",
      "loss= 3.655367851257324 for iteration= 78\n",
      "loss= 3.6391239166259766 for iteration= 79\n",
      "loss= 3.623178243637085 for iteration= 80\n",
      "loss= 3.607516050338745 for iteration= 81\n",
      "loss= 3.592125654220581 for iteration= 82\n",
      "loss= 3.576998233795166 for iteration= 83\n",
      "loss= 3.562126874923706 for iteration= 84\n",
      "loss= 3.547508716583252 for iteration= 85\n",
      "loss= 3.5331430435180664 for iteration= 86\n",
      "loss= 3.519031286239624 for iteration= 87\n",
      "loss= 3.505176067352295 for iteration= 88\n",
      "loss= 3.491582155227661 for iteration= 89\n",
      "loss= 3.478254795074463 for iteration= 90\n",
      "loss= 3.4651997089385986 for iteration= 91\n",
      "loss= 3.452420711517334 for iteration= 92\n",
      "loss= 3.439924478530884 for iteration= 93\n",
      "loss= 3.42771315574646 for iteration= 94\n",
      "loss= 3.4157907962799072 for iteration= 95\n",
      "loss= 3.404158592224121 for iteration= 96\n",
      "loss= 3.392817974090576 for iteration= 97\n",
      "loss= 3.3817691802978516 for iteration= 98\n",
      "loss= 3.3710107803344727 for iteration= 99\n",
      "loss= 3.360541343688965 for iteration= 100\n",
      "loss= 3.350356340408325 for iteration= 101\n",
      "loss= 3.3404526710510254 for iteration= 102\n",
      "loss= 3.3308253288269043 for iteration= 103\n",
      "loss= 3.3214681148529053 for iteration= 104\n",
      "loss= 3.3123738765716553 for iteration= 105\n",
      "loss= 3.303534984588623 for iteration= 106\n",
      "loss= 3.294943332672119 for iteration= 107\n",
      "loss= 3.286588430404663 for iteration= 108\n",
      "loss= 3.2784624099731445 for iteration= 109\n",
      "loss= 3.270554780960083 for iteration= 110\n",
      "loss= 3.2628567218780518 for iteration= 111\n",
      "loss= 3.255357265472412 for iteration= 112\n",
      "loss= 3.2480478286743164 for iteration= 113\n",
      "loss= 3.2409188747406006 for iteration= 114\n",
      "loss= 3.233961582183838 for iteration= 115\n",
      "loss= 3.227167844772339 for iteration= 116\n",
      "loss= 3.220529317855835 for iteration= 117\n",
      "loss= 3.214038372039795 for iteration= 118\n",
      "loss= 3.207689046859741 for iteration= 119\n",
      "loss= 3.20147442817688 for iteration= 120\n",
      "loss= 3.1953885555267334 for iteration= 121\n",
      "loss= 3.1894264221191406 for iteration= 122\n",
      "loss= 3.1835830211639404 for iteration= 123\n",
      "loss= 3.177854061126709 for iteration= 124\n",
      "loss= 3.1722347736358643 for iteration= 125\n",
      "loss= 3.166722297668457 for iteration= 126\n",
      "loss= 3.1613123416900635 for iteration= 127\n",
      "loss= 3.1560020446777344 for iteration= 128\n",
      "loss= 3.1507885456085205 for iteration= 129\n",
      "loss= 3.1456692218780518 for iteration= 130\n",
      "loss= 3.140641689300537 for iteration= 131\n",
      "loss= 3.135702610015869 for iteration= 132\n",
      "loss= 3.1308515071868896 for iteration= 133\n",
      "loss= 3.126084327697754 for iteration= 134\n",
      "loss= 3.121399164199829 for iteration= 135\n",
      "loss= 3.116795778274536 for iteration= 136\n",
      "loss= 3.1122710704803467 for iteration= 137\n",
      "loss= 3.107822895050049 for iteration= 138\n",
      "loss= 3.1034505367279053 for iteration= 139\n",
      "loss= 3.0991508960723877 for iteration= 140\n",
      "loss= 3.094923496246338 for iteration= 141\n",
      "loss= 3.0907657146453857 for iteration= 142\n",
      "loss= 3.0866761207580566 for iteration= 143\n",
      "loss= 3.082653522491455 for iteration= 144\n",
      "loss= 3.078695774078369 for iteration= 145\n",
      "loss= 3.074801445007324 for iteration= 146\n",
      "loss= 3.0709688663482666 for iteration= 147\n",
      "loss= 3.0671958923339844 for iteration= 148\n",
      "loss= 3.0634818077087402 for iteration= 149\n",
      "loss= 3.0598247051239014 for iteration= 150\n",
      "loss= 3.0562233924865723 for iteration= 151\n",
      "loss= 3.052675485610962 for iteration= 152\n",
      "loss= 3.049179792404175 for iteration= 153\n",
      "loss= 3.0457358360290527 for iteration= 154\n",
      "loss= 3.0423407554626465 for iteration= 155\n",
      "loss= 3.0389935970306396 for iteration= 156\n",
      "loss= 3.035693883895874 for iteration= 157\n",
      "loss= 3.0324392318725586 for iteration= 158\n",
      "loss= 3.029228925704956 for iteration= 159\n",
      "loss= 3.026061773300171 for iteration= 160\n",
      "loss= 3.0229358673095703 for iteration= 161\n",
      "loss= 3.019850730895996 for iteration= 162\n",
      "loss= 3.0168049335479736 for iteration= 163\n",
      "loss= 3.0137975215911865 for iteration= 164\n",
      "loss= 3.0108275413513184 for iteration= 165\n",
      "loss= 3.0078935623168945 for iteration= 166\n",
      "loss= 3.0049948692321777 for iteration= 167\n",
      "loss= 3.0021307468414307 for iteration= 168\n",
      "loss= 2.9992997646331787 for iteration= 169\n",
      "loss= 2.996501922607422 for iteration= 170\n",
      "loss= 2.9937350749969482 for iteration= 171\n",
      "loss= 2.990999698638916 for iteration= 172\n",
      "loss= 2.9882941246032715 for iteration= 173\n",
      "loss= 2.9856183528900146 for iteration= 174\n",
      "loss= 2.982971668243408 for iteration= 175\n",
      "loss= 2.9803524017333984 for iteration= 176\n",
      "loss= 2.9777603149414062 for iteration= 177\n",
      "loss= 2.97519588470459 for iteration= 178\n",
      "loss= 2.9726569652557373 for iteration= 179\n",
      "loss= 2.970144033432007 for iteration= 180\n",
      "loss= 2.967656135559082 for iteration= 181\n",
      "loss= 2.9651927947998047 for iteration= 182\n",
      "loss= 2.9627532958984375 for iteration= 183\n",
      "loss= 2.9603376388549805 for iteration= 184\n",
      "loss= 2.957944631576538 for iteration= 185\n",
      "loss= 2.9555747509002686 for iteration= 186\n",
      "loss= 2.9532268047332764 for iteration= 187\n",
      "loss= 2.9509007930755615 for iteration= 188\n",
      "loss= 2.9485960006713867 for iteration= 189\n",
      "loss= 2.946312427520752 for iteration= 190\n",
      "loss= 2.9440488815307617 for iteration= 191\n",
      "loss= 2.9418060779571533 for iteration= 192\n",
      "loss= 2.9395833015441895 for iteration= 193\n",
      "loss= 2.937380075454712 for iteration= 194\n",
      "loss= 2.9351959228515625 for iteration= 195\n",
      "loss= 2.933030843734741 for iteration= 196\n",
      "loss= 2.9308841228485107 for iteration= 197\n",
      "loss= 2.9287562370300293 for iteration= 198\n",
      "loss= 2.9266462326049805 for iteration= 199\n",
      "loss= 2.924553632736206 for iteration= 200\n",
      "loss= 2.9224789142608643 for iteration= 201\n",
      "loss= 2.920421838760376 for iteration= 202\n",
      "loss= 2.918381452560425 for iteration= 203\n",
      "loss= 2.9163575172424316 for iteration= 204\n",
      "loss= 2.9143502712249756 for iteration= 205\n",
      "loss= 2.9123594760894775 for iteration= 206\n",
      "loss= 2.9103848934173584 for iteration= 207\n",
      "loss= 2.90842604637146 for iteration= 208\n",
      "loss= 2.9064829349517822 for iteration= 209\n",
      "loss= 2.904555082321167 for iteration= 210\n",
      "loss= 2.9026424884796143 for iteration= 211\n",
      "loss= 2.900744915008545 for iteration= 212\n",
      "loss= 2.898862838745117 for iteration= 213\n",
      "loss= 2.8969950675964355 for iteration= 214\n",
      "loss= 2.895141124725342 for iteration= 215\n",
      "loss= 2.8933024406433105 for iteration= 216\n",
      "loss= 2.891477346420288 for iteration= 217\n",
      "loss= 2.8896665573120117 for iteration= 218\n",
      "loss= 2.8878695964813232 for iteration= 219\n",
      "loss= 2.8860864639282227 for iteration= 220\n",
      "loss= 2.8843166828155518 for iteration= 221\n",
      "loss= 2.8825597763061523 for iteration= 222\n",
      "loss= 2.880816698074341 for iteration= 223\n",
      "loss= 2.87908673286438 for iteration= 224\n",
      "loss= 2.8773696422576904 for iteration= 225\n",
      "loss= 2.8756654262542725 for iteration= 226\n",
      "loss= 2.873973846435547 for iteration= 227\n",
      "loss= 2.8722946643829346 for iteration= 228\n",
      "loss= 2.8706278800964355 for iteration= 229\n",
      "loss= 2.868973731994629 for iteration= 230\n",
      "loss= 2.867331027984619 for iteration= 231\n",
      "loss= 2.865701198577881 for iteration= 232\n",
      "loss= 2.8640828132629395 for iteration= 233\n",
      "loss= 2.8624770641326904 for iteration= 234\n",
      "loss= 2.8608827590942383 for iteration= 235\n",
      "loss= 2.8592991828918457 for iteration= 236\n",
      "loss= 2.8577280044555664 for iteration= 237\n",
      "loss= 2.856168031692505 for iteration= 238\n",
      "loss= 2.8546197414398193 for iteration= 239\n",
      "loss= 2.853081703186035 for iteration= 240\n",
      "loss= 2.851555347442627 for iteration= 241\n",
      "loss= 2.8500404357910156 for iteration= 242\n",
      "loss= 2.848536491394043 for iteration= 243\n",
      "loss= 2.847043037414551 for iteration= 244\n",
      "loss= 2.8455605506896973 for iteration= 245\n",
      "loss= 2.844088554382324 for iteration= 246\n",
      "loss= 2.84262752532959 for iteration= 247\n",
      "loss= 2.841176748275757 for iteration= 248\n",
      "loss= 2.8397367000579834 for iteration= 249\n",
      "loss= 2.8383071422576904 for iteration= 250\n",
      "loss= 2.8368875980377197 for iteration= 251\n",
      "loss= 2.8354785442352295 for iteration= 252\n",
      "loss= 2.8340792655944824 for iteration= 253\n",
      "loss= 2.832690954208374 for iteration= 254\n",
      "loss= 2.8313121795654297 for iteration= 255\n",
      "loss= 2.829943895339966 for iteration= 256\n",
      "loss= 2.828585386276245 for iteration= 257\n",
      "loss= 2.8272361755371094 for iteration= 258\n",
      "loss= 2.825897455215454 for iteration= 259\n",
      "loss= 2.824568510055542 for iteration= 260\n",
      "loss= 2.823249101638794 for iteration= 261\n",
      "loss= 2.821939468383789 for iteration= 262\n",
      "loss= 2.8206396102905273 for iteration= 263\n",
      "loss= 2.8193485736846924 for iteration= 264\n",
      "loss= 2.818067789077759 for iteration= 265\n",
      "loss= 2.816796064376831 for iteration= 266\n",
      "loss= 2.8155338764190674 for iteration= 267\n",
      "loss= 2.8142812252044678 for iteration= 268\n",
      "loss= 2.813037872314453 for iteration= 269\n",
      "loss= 2.8118038177490234 for iteration= 270\n",
      "loss= 2.8105788230895996 for iteration= 271\n",
      "loss= 2.8093626499176025 for iteration= 272\n",
      "loss= 2.8081560134887695 for iteration= 273\n",
      "loss= 2.8069586753845215 for iteration= 274\n",
      "loss= 2.805769443511963 for iteration= 275\n",
      "loss= 2.8045897483825684 for iteration= 276\n",
      "loss= 2.8034191131591797 for iteration= 277\n",
      "loss= 2.8022570610046387 for iteration= 278\n",
      "loss= 2.8011038303375244 for iteration= 279\n",
      "loss= 2.799959421157837 for iteration= 280\n",
      "loss= 2.798823356628418 for iteration= 281\n",
      "loss= 2.797696352005005 for iteration= 282\n",
      "loss= 2.7965779304504395 for iteration= 283\n",
      "loss= 2.7954678535461426 for iteration= 284\n",
      "loss= 2.7943661212921143 for iteration= 285\n",
      "loss= 2.7932727336883545 for iteration= 286\n",
      "loss= 2.7921879291534424 for iteration= 287\n",
      "loss= 2.7911107540130615 for iteration= 288\n",
      "loss= 2.7900428771972656 for iteration= 289\n",
      "loss= 2.788982391357422 for iteration= 290\n",
      "loss= 2.7879302501678467 for iteration= 291\n",
      "loss= 2.786885976791382 for iteration= 292\n",
      "loss= 2.7858498096466064 for iteration= 293\n",
      "loss= 2.7848217487335205 for iteration= 294\n",
      "loss= 2.7838010787963867 for iteration= 295\n",
      "loss= 2.7827885150909424 for iteration= 296\n",
      "loss= 2.78178334236145 for iteration= 297\n",
      "loss= 2.78078556060791 for iteration= 298\n",
      "loss= 2.7797961235046387 for iteration= 299\n",
      "loss= 2.7788140773773193 for iteration= 300\n",
      "loss= 2.777839183807373 for iteration= 301\n",
      "loss= 2.776871681213379 for iteration= 302\n",
      "loss= 2.775911331176758 for iteration= 303\n",
      "loss= 2.774958610534668 for iteration= 304\n",
      "loss= 2.774012565612793 for iteration= 305\n",
      "loss= 2.773073673248291 for iteration= 306\n",
      "loss= 2.7721426486968994 for iteration= 307\n",
      "loss= 2.7712175846099854 for iteration= 308\n",
      "loss= 2.7702994346618652 for iteration= 309\n",
      "loss= 2.7693889141082764 for iteration= 310\n",
      "loss= 2.768484354019165 for iteration= 311\n",
      "loss= 2.7675867080688477 for iteration= 312\n",
      "loss= 2.766695976257324 for iteration= 313\n",
      "loss= 2.7658116817474365 for iteration= 314\n",
      "loss= 2.7649335861206055 for iteration= 315\n",
      "loss= 2.764061689376831 for iteration= 316\n",
      "loss= 2.7631967067718506 for iteration= 317\n",
      "loss= 2.7623376846313477 for iteration= 318\n",
      "loss= 2.7614848613739014 for iteration= 319\n",
      "loss= 2.7606382369995117 for iteration= 320\n",
      "loss= 2.7597978115081787 for iteration= 321\n",
      "loss= 2.758962631225586 for iteration= 322\n",
      "loss= 2.758134365081787 for iteration= 323\n",
      "loss= 2.7573113441467285 for iteration= 324\n",
      "loss= 2.7564942836761475 for iteration= 325\n",
      "loss= 2.7556827068328857 for iteration= 326\n",
      "loss= 2.7548770904541016 for iteration= 327\n",
      "loss= 2.754077196121216 for iteration= 328\n",
      "loss= 2.7532825469970703 for iteration= 329\n",
      "loss= 2.752493143081665 for iteration= 330\n",
      "loss= 2.751709461212158 for iteration= 331\n",
      "loss= 2.7509307861328125 for iteration= 332\n",
      "loss= 2.7501578330993652 for iteration= 333\n",
      "loss= 2.749389410018921 for iteration= 334\n",
      "loss= 2.748626708984375 for iteration= 335\n",
      "loss= 2.7478690147399902 for iteration= 336\n",
      "loss= 2.7471160888671875 for iteration= 337\n",
      "loss= 2.746368646621704 for iteration= 338\n",
      "loss= 2.7456259727478027 for iteration= 339\n",
      "loss= 2.744887590408325 for iteration= 340\n",
      "loss= 2.744154691696167 for iteration= 341\n",
      "loss= 2.7434260845184326 for iteration= 342\n",
      "loss= 2.7427022457122803 for iteration= 343\n",
      "loss= 2.741983413696289 for iteration= 344\n",
      "loss= 2.7412686347961426 for iteration= 345\n",
      "loss= 2.7405588626861572 for iteration= 346\n",
      "loss= 2.7398531436920166 for iteration= 347\n",
      "loss= 2.739152193069458 for iteration= 348\n",
      "loss= 2.738455295562744 for iteration= 349\n",
      "loss= 2.737762928009033 for iteration= 350\n",
      "loss= 2.737074375152588 for iteration= 351\n",
      "loss= 2.7363908290863037 for iteration= 352\n",
      "loss= 2.735710620880127 for iteration= 353\n",
      "loss= 2.7350354194641113 for iteration= 354\n",
      "loss= 2.734363555908203 for iteration= 355\n",
      "loss= 2.7336959838867188 for iteration= 356\n",
      "loss= 2.733032464981079 for iteration= 357\n",
      "loss= 2.732372760772705 for iteration= 358\n",
      "loss= 2.731717109680176 for iteration= 359\n",
      "loss= 2.731065511703491 for iteration= 360\n",
      "loss= 2.730417251586914 for iteration= 361\n",
      "loss= 2.7297728061676025 for iteration= 362\n",
      "loss= 2.7291324138641357 for iteration= 363\n",
      "loss= 2.7284953594207764 for iteration= 364\n",
      "loss= 2.727862596511841 for iteration= 365\n",
      "loss= 2.7272326946258545 for iteration= 366\n",
      "loss= 2.726606845855713 for iteration= 367\n",
      "loss= 2.7259843349456787 for iteration= 368\n",
      "loss= 2.725365400314331 for iteration= 369\n",
      "loss= 2.72475004196167 for iteration= 370\n",
      "loss= 2.724137783050537 for iteration= 371\n",
      "loss= 2.723529100418091 for iteration= 372\n",
      "loss= 2.722923517227173 for iteration= 373\n",
      "loss= 2.7223215103149414 for iteration= 374\n",
      "loss= 2.7217228412628174 for iteration= 375\n",
      "loss= 2.7211272716522217 for iteration= 376\n",
      "loss= 2.7205352783203125 for iteration= 377\n",
      "loss= 2.7199463844299316 for iteration= 378\n",
      "loss= 2.7193603515625 for iteration= 379\n",
      "loss= 2.7187771797180176 for iteration= 380\n",
      "loss= 2.718197822570801 for iteration= 381\n",
      "loss= 2.717621088027954 for iteration= 382\n",
      "loss= 2.7170474529266357 for iteration= 383\n",
      "loss= 2.7164766788482666 for iteration= 384\n",
      "loss= 2.715909242630005 for iteration= 385\n",
      "loss= 2.7153446674346924 for iteration= 386\n",
      "loss= 2.714782953262329 for iteration= 387\n",
      "loss= 2.714223623275757 for iteration= 388\n",
      "loss= 2.71366810798645 for iteration= 389\n",
      "loss= 2.7131149768829346 for iteration= 390\n",
      "loss= 2.712564468383789 for iteration= 391\n",
      "loss= 2.7120165824890137 for iteration= 392\n",
      "loss= 2.7114717960357666 for iteration= 393\n",
      "loss= 2.710930109024048 for iteration= 394\n",
      "loss= 2.71039080619812 for iteration= 395\n",
      "loss= 2.7098541259765625 for iteration= 396\n",
      "loss= 2.7093207836151123 for iteration= 397\n",
      "loss= 2.708789348602295 for iteration= 398\n",
      "loss= 2.7082607746124268 for iteration= 399\n",
      "loss= 2.707735061645508 for iteration= 400\n",
      "loss= 2.707212209701538 for iteration= 401\n",
      "loss= 2.7066919803619385 for iteration= 402\n",
      "loss= 2.70617413520813 for iteration= 403\n",
      "loss= 2.7056596279144287 for iteration= 404\n",
      "loss= 2.7051475048065186 for iteration= 405\n",
      "loss= 2.704639196395874 for iteration= 406\n",
      "loss= 2.7041335105895996 for iteration= 407\n",
      "loss= 2.703631639480591 for iteration= 408\n",
      "loss= 2.7031335830688477 for iteration= 409\n",
      "loss= 2.7026402950286865 for iteration= 410\n",
      "loss= 2.702151298522949 for iteration= 411\n",
      "loss= 2.701669692993164 for iteration= 412\n",
      "loss= 2.7011945247650146 for iteration= 413\n",
      "loss= 2.700730800628662 for iteration= 414\n",
      "loss= 2.7002763748168945 for iteration= 415\n",
      "loss= 2.699842929840088 for iteration= 416\n",
      "loss= 2.699422836303711 for iteration= 417\n",
      "loss= 2.6990444660186768 for iteration= 418\n",
      "loss= 2.6986825466156006 for iteration= 419\n",
      "loss= 2.698408365249634 for iteration= 420\n",
      "loss= 2.6981470584869385 for iteration= 421\n",
      "loss= 2.6980745792388916 for iteration= 422\n",
      "loss= 2.697981595993042 for iteration= 423\n",
      "loss= 2.69830322265625 for iteration= 424\n",
      "loss= 2.698467493057251 for iteration= 425\n",
      "loss= 2.6995532512664795 for iteration= 426\n",
      "loss= 2.7000372409820557 for iteration= 427\n",
      "loss= 2.7025437355041504 for iteration= 428\n",
      "loss= 2.7031946182250977 for iteration= 429\n",
      "loss= 2.7081046104431152 for iteration= 430\n",
      "loss= 2.7081339359283447 for iteration= 431\n",
      "loss= 2.716459274291992 for iteration= 432\n",
      "loss= 2.714076280593872 for iteration= 433\n",
      "loss= 2.726008415222168 for iteration= 434\n",
      "loss= 2.7191600799560547 for iteration= 435\n",
      "loss= 2.733490228652954 for iteration= 436\n",
      "loss= 2.721874713897705 for iteration= 437\n",
      "loss= 2.7368881702423096 for iteration= 438\n",
      "loss= 2.7223501205444336 for iteration= 439\n",
      "loss= 2.737090826034546 for iteration= 440\n",
      "loss= 2.7215681076049805 for iteration= 441\n",
      "loss= 2.735809803009033 for iteration= 442\n",
      "loss= 2.72029185295105 for iteration= 443\n",
      "loss= 2.734036922454834 for iteration= 444\n",
      "loss= 2.7188773155212402 for iteration= 445\n",
      "loss= 2.732194423675537 for iteration= 446\n",
      "loss= 2.717442512512207 for iteration= 447\n",
      "loss= 2.7303733825683594 for iteration= 448\n",
      "loss= 2.7160513401031494 for iteration= 449\n",
      "loss= 2.7286407947540283 for iteration= 450\n",
      "loss= 2.7147016525268555 for iteration= 451\n",
      "loss= 2.7269883155822754 for iteration= 452\n",
      "loss= 2.713395118713379 for iteration= 453\n",
      "loss= 2.7253992557525635 for iteration= 454\n",
      "loss= 2.712120532989502 for iteration= 455\n",
      "loss= 2.723851203918457 for iteration= 456\n",
      "loss= 2.7108840942382812 for iteration= 457\n",
      "loss= 2.7223758697509766 for iteration= 458\n",
      "loss= 2.7096779346466064 for iteration= 459\n",
      "loss= 2.72092866897583 for iteration= 460\n",
      "loss= 2.708494186401367 for iteration= 461\n",
      "loss= 2.719529151916504 for iteration= 462\n",
      "loss= 2.7073376178741455 for iteration= 463\n",
      "loss= 2.7181577682495117 for iteration= 464\n",
      "loss= 2.7061867713928223 for iteration= 465\n",
      "loss= 2.716794013977051 for iteration= 466\n",
      "loss= 2.705054521560669 for iteration= 467\n",
      "loss= 2.7154629230499268 for iteration= 468\n",
      "loss= 2.7039453983306885 for iteration= 469\n",
      "loss= 2.714160203933716 for iteration= 470\n",
      "loss= 2.702848434448242 for iteration= 471\n",
      "loss= 2.7128782272338867 for iteration= 472\n",
      "loss= 2.7017619609832764 for iteration= 473\n",
      "loss= 2.711620330810547 for iteration= 474\n",
      "loss= 2.7006990909576416 for iteration= 475\n",
      "loss= 2.7103776931762695 for iteration= 476\n",
      "loss= 2.699638843536377 for iteration= 477\n",
      "loss= 2.7091519832611084 for iteration= 478\n",
      "loss= 2.698592185974121 for iteration= 479\n",
      "loss= 2.7079355716705322 for iteration= 480\n",
      "loss= 2.697561740875244 for iteration= 481\n",
      "loss= 2.706744432449341 for iteration= 482\n",
      "loss= 2.6965370178222656 for iteration= 483\n",
      "loss= 2.7055606842041016 for iteration= 484\n",
      "loss= 2.695525884628296 for iteration= 485\n",
      "loss= 2.7044034004211426 for iteration= 486\n",
      "loss= 2.6945223808288574 for iteration= 487\n",
      "loss= 2.7032432556152344 for iteration= 488\n",
      "loss= 2.693528175354004 for iteration= 489\n",
      "loss= 2.7021005153656006 for iteration= 490\n",
      "loss= 2.69254994392395 for iteration= 491\n",
      "loss= 2.700979232788086 for iteration= 492\n",
      "loss= 2.691575527191162 for iteration= 493\n",
      "loss= 2.6998658180236816 for iteration= 494\n",
      "loss= 2.690610408782959 for iteration= 495\n",
      "loss= 2.6987621784210205 for iteration= 496\n",
      "loss= 2.689652442932129 for iteration= 497\n",
      "loss= 2.697666645050049 for iteration= 498\n",
      "loss= 2.6887047290802 for iteration= 499\n",
      "loss= 2.696587085723877 for iteration= 500\n",
      "loss= 2.6877686977386475 for iteration= 501\n",
      "loss= 2.6955182552337646 for iteration= 502\n",
      "loss= 2.6868350505828857 for iteration= 503\n",
      "loss= 2.694456100463867 for iteration= 504\n",
      "loss= 2.6859123706817627 for iteration= 505\n",
      "loss= 2.693399667739868 for iteration= 506\n",
      "loss= 2.6849920749664307 for iteration= 507\n",
      "loss= 2.6923599243164062 for iteration= 508\n",
      "loss= 2.684079885482788 for iteration= 509\n",
      "loss= 2.6913232803344727 for iteration= 510\n",
      "loss= 2.6831765174865723 for iteration= 511\n",
      "loss= 2.690294027328491 for iteration= 512\n",
      "loss= 2.6822776794433594 for iteration= 513\n",
      "loss= 2.6892714500427246 for iteration= 514\n",
      "loss= 2.6813857555389404 for iteration= 515\n",
      "loss= 2.688264846801758 for iteration= 516\n",
      "loss= 2.680495500564575 for iteration= 517\n",
      "loss= 2.687252998352051 for iteration= 518\n",
      "loss= 2.679617166519165 for iteration= 519\n",
      "loss= 2.6862635612487793 for iteration= 520\n",
      "loss= 2.6787493228912354 for iteration= 521\n",
      "loss= 2.6852755546569824 for iteration= 522\n",
      "loss= 2.677877902984619 for iteration= 523\n",
      "loss= 2.6842904090881348 for iteration= 524\n",
      "loss= 2.6770148277282715 for iteration= 525\n",
      "loss= 2.683310031890869 for iteration= 526\n",
      "loss= 2.676161289215088 for iteration= 527\n",
      "loss= 2.682353973388672 for iteration= 528\n",
      "loss= 2.6753151416778564 for iteration= 529\n",
      "loss= 2.6813950538635254 for iteration= 530\n",
      "loss= 2.674471378326416 for iteration= 531\n",
      "loss= 2.6804378032684326 for iteration= 532\n",
      "loss= 2.673632860183716 for iteration= 533\n",
      "loss= 2.679494619369507 for iteration= 534\n",
      "loss= 2.6728007793426514 for iteration= 535\n",
      "loss= 2.6785590648651123 for iteration= 536\n",
      "loss= 2.6719751358032227 for iteration= 537\n",
      "loss= 2.6776278018951416 for iteration= 538\n",
      "loss= 2.6711511611938477 for iteration= 539\n",
      "loss= 2.676703691482544 for iteration= 540\n",
      "loss= 2.670335292816162 for iteration= 541\n",
      "loss= 2.6757800579071045 for iteration= 542\n",
      "loss= 2.6695215702056885 for iteration= 543\n",
      "loss= 2.674863815307617 for iteration= 544\n",
      "loss= 2.6687135696411133 for iteration= 545\n",
      "loss= 2.673954725265503 for iteration= 546\n",
      "loss= 2.6679155826568604 for iteration= 547\n",
      "loss= 2.6730599403381348 for iteration= 548\n",
      "loss= 2.667123317718506 for iteration= 549\n",
      "loss= 2.6721689701080322 for iteration= 550\n",
      "loss= 2.6663365364074707 for iteration= 551\n",
      "loss= 2.67128586769104 for iteration= 552\n",
      "loss= 2.665555238723755 for iteration= 553\n",
      "loss= 2.6704156398773193 for iteration= 554\n",
      "loss= 2.664781093597412 for iteration= 555\n",
      "loss= 2.6695430278778076 for iteration= 556\n",
      "loss= 2.6640045642852783 for iteration= 557\n",
      "loss= 2.6686723232269287 for iteration= 558\n",
      "loss= 2.663233757019043 for iteration= 559\n",
      "loss= 2.667808771133423 for iteration= 560\n",
      "loss= 2.6624667644500732 for iteration= 561\n",
      "loss= 2.6669485569000244 for iteration= 562\n",
      "loss= 2.66170597076416 for iteration= 563\n",
      "loss= 2.6660959720611572 for iteration= 564\n",
      "loss= 2.6609508991241455 for iteration= 565\n",
      "loss= 2.6652510166168213 for iteration= 566\n",
      "loss= 2.66019868850708 for iteration= 567\n",
      "loss= 2.6644108295440674 for iteration= 568\n",
      "loss= 2.659450054168701 for iteration= 569\n",
      "loss= 2.663573741912842 for iteration= 570\n",
      "loss= 2.6587047576904297 for iteration= 571\n",
      "loss= 2.662743091583252 for iteration= 572\n",
      "loss= 2.6579666137695312 for iteration= 573\n",
      "loss= 2.6619174480438232 for iteration= 574\n",
      "loss= 2.657233238220215 for iteration= 575\n",
      "loss= 2.6610989570617676 for iteration= 576\n",
      "loss= 2.656505584716797 for iteration= 577\n",
      "loss= 2.660288095474243 for iteration= 578\n",
      "loss= 2.655780792236328 for iteration= 579\n",
      "loss= 2.6594786643981934 for iteration= 580\n",
      "loss= 2.6550586223602295 for iteration= 581\n",
      "loss= 2.658672332763672 for iteration= 582\n",
      "loss= 2.6543431282043457 for iteration= 583\n",
      "loss= 2.65787935256958 for iteration= 584\n",
      "loss= 2.6536338329315186 for iteration= 585\n",
      "loss= 2.657089948654175 for iteration= 586\n",
      "loss= 2.6529273986816406 for iteration= 587\n",
      "loss= 2.6563034057617188 for iteration= 588\n",
      "loss= 2.6522252559661865 for iteration= 589\n",
      "loss= 2.6555256843566895 for iteration= 590\n",
      "loss= 2.6515285968780518 for iteration= 591\n",
      "loss= 2.654750347137451 for iteration= 592\n",
      "loss= 2.650831937789917 for iteration= 593\n",
      "loss= 2.653977155685425 for iteration= 594\n",
      "loss= 2.6501405239105225 for iteration= 595\n",
      "loss= 2.6532111167907715 for iteration= 596\n",
      "loss= 2.649454116821289 for iteration= 597\n",
      "loss= 2.6524507999420166 for iteration= 598\n",
      "loss= 2.648775815963745 for iteration= 599\n",
      "loss= 2.6516988277435303 for iteration= 600\n",
      "loss= 2.6480982303619385 for iteration= 601\n",
      "loss= 2.6509501934051514 for iteration= 602\n",
      "loss= 2.647423505783081 for iteration= 603\n",
      "loss= 2.6502022743225098 for iteration= 604\n",
      "loss= 2.646751880645752 for iteration= 605\n",
      "loss= 2.649461269378662 for iteration= 606\n",
      "loss= 2.646087408065796 for iteration= 607\n",
      "loss= 2.6487269401550293 for iteration= 608\n",
      "loss= 2.64542555809021 for iteration= 609\n",
      "loss= 2.6479973793029785 for iteration= 610\n",
      "loss= 2.6447675228118896 for iteration= 611\n",
      "loss= 2.6472713947296143 for iteration= 612\n",
      "loss= 2.6441140174865723 for iteration= 613\n",
      "loss= 2.646552085876465 for iteration= 614\n",
      "loss= 2.643463611602783 for iteration= 615\n",
      "loss= 2.6458358764648438 for iteration= 616\n",
      "loss= 2.642820119857788 for iteration= 617\n",
      "loss= 2.6451263427734375 for iteration= 618\n",
      "loss= 2.6421775817871094 for iteration= 619\n",
      "loss= 2.6444201469421387 for iteration= 620\n",
      "loss= 2.64153790473938 for iteration= 621\n",
      "loss= 2.6437177658081055 for iteration= 622\n",
      "loss= 2.640903949737549 for iteration= 623\n",
      "loss= 2.643021583557129 for iteration= 624\n",
      "loss= 2.640272855758667 for iteration= 625\n",
      "loss= 2.642329692840576 for iteration= 626\n",
      "loss= 2.63964581489563 for iteration= 627\n",
      "loss= 2.6416432857513428 for iteration= 628\n",
      "loss= 2.639023542404175 for iteration= 629\n",
      "loss= 2.6409616470336914 for iteration= 630\n",
      "loss= 2.6384034156799316 for iteration= 631\n",
      "loss= 2.6402828693389893 for iteration= 632\n",
      "loss= 2.6377880573272705 for iteration= 633\n",
      "loss= 2.6396100521087646 for iteration= 634\n",
      "loss= 2.6371757984161377 for iteration= 635\n",
      "loss= 2.6389412879943848 for iteration= 636\n",
      "loss= 2.6365654468536377 for iteration= 637\n",
      "loss= 2.638274908065796 for iteration= 638\n",
      "loss= 2.635960340499878 for iteration= 639\n",
      "loss= 2.6376147270202637 for iteration= 640\n",
      "loss= 2.635359048843384 for iteration= 641\n",
      "loss= 2.636958360671997 for iteration= 642\n",
      "loss= 2.634761333465576 for iteration= 643\n",
      "loss= 2.636307954788208 for iteration= 644\n",
      "loss= 2.6341652870178223 for iteration= 645\n",
      "loss= 2.635660409927368 for iteration= 646\n",
      "loss= 2.6335761547088623 for iteration= 647\n",
      "loss= 2.6350200176239014 for iteration= 648\n",
      "loss= 2.6329891681671143 for iteration= 649\n",
      "loss= 2.634382724761963 for iteration= 650\n",
      "loss= 2.6324069499969482 for iteration= 651\n",
      "loss= 2.6337521076202393 for iteration= 652\n",
      "loss= 2.6318295001983643 for iteration= 653\n",
      "loss= 2.6331260204315186 for iteration= 654\n",
      "loss= 2.631253480911255 for iteration= 655\n",
      "loss= 2.6325020790100098 for iteration= 656\n",
      "loss= 2.630680561065674 for iteration= 657\n",
      "loss= 2.631883382797241 for iteration= 658\n",
      "loss= 2.6301116943359375 for iteration= 659\n",
      "loss= 2.6312692165374756 for iteration= 660\n",
      "loss= 2.629546880722046 for iteration= 661\n",
      "loss= 2.630660057067871 for iteration= 662\n",
      "loss= 2.6289865970611572 for iteration= 663\n",
      "loss= 2.6300549507141113 for iteration= 664\n",
      "loss= 2.628427743911743 for iteration= 665\n",
      "loss= 2.6294522285461426 for iteration= 666\n",
      "loss= 2.627872943878174 for iteration= 667\n",
      "loss= 2.628856897354126 for iteration= 668\n",
      "loss= 2.62732195854187 for iteration= 669\n",
      "loss= 2.628263473510742 for iteration= 670\n",
      "loss= 2.6267740726470947 for iteration= 671\n",
      "loss= 2.6276767253875732 for iteration= 672\n",
      "loss= 2.6262316703796387 for iteration= 673\n",
      "loss= 2.6270952224731445 for iteration= 674\n",
      "loss= 2.6256914138793945 for iteration= 675\n",
      "loss= 2.626516580581665 for iteration= 676\n",
      "loss= 2.625155210494995 for iteration= 677\n",
      "loss= 2.625941276550293 for iteration= 678\n",
      "loss= 2.624619960784912 for iteration= 679\n",
      "loss= 2.6253697872161865 for iteration= 680\n",
      "loss= 2.6240899562835693 for iteration= 681\n",
      "loss= 2.624803304672241 for iteration= 682\n",
      "loss= 2.6235625743865967 for iteration= 683\n",
      "loss= 2.6242401599884033 for iteration= 684\n",
      "loss= 2.6230390071868896 for iteration= 685\n",
      "loss= 2.6236822605133057 for iteration= 686\n",
      "loss= 2.622518301010132 for iteration= 687\n",
      "loss= 2.6231284141540527 for iteration= 688\n",
      "loss= 2.6220011711120605 for iteration= 689\n",
      "loss= 2.6225783824920654 for iteration= 690\n",
      "loss= 2.621487617492676 for iteration= 691\n",
      "loss= 2.6220321655273438 for iteration= 692\n",
      "loss= 2.620978355407715 for iteration= 693\n",
      "loss= 2.6214911937713623 for iteration= 694\n",
      "loss= 2.6204705238342285 for iteration= 695\n",
      "loss= 2.620953321456909 for iteration= 696\n",
      "loss= 2.619966745376587 for iteration= 697\n",
      "loss= 2.6204192638397217 for iteration= 698\n",
      "loss= 2.619466781616211 for iteration= 699\n",
      "loss= 2.619891405105591 for iteration= 700\n",
      "loss= 2.6189701557159424 for iteration= 701\n",
      "loss= 2.619365930557251 for iteration= 702\n",
      "loss= 2.618476390838623 for iteration= 703\n",
      "loss= 2.618845224380493 for iteration= 704\n",
      "loss= 2.617985963821411 for iteration= 705\n",
      "loss= 2.61832857131958 for iteration= 706\n",
      "loss= 2.617499351501465 for iteration= 707\n",
      "loss= 2.6178159713745117 for iteration= 708\n",
      "loss= 2.617016077041626 for iteration= 709\n",
      "loss= 2.6173083782196045 for iteration= 710\n",
      "loss= 2.6165363788604736 for iteration= 711\n",
      "loss= 2.616804838180542 for iteration= 712\n",
      "loss= 2.616060256958008 for iteration= 713\n",
      "loss= 2.616305112838745 for iteration= 714\n",
      "loss= 2.615586996078491 for iteration= 715\n",
      "loss= 2.615809440612793 for iteration= 716\n",
      "loss= 2.615117311477661 for iteration= 717\n",
      "loss= 2.6153180599212646 for iteration= 718\n",
      "loss= 2.6146507263183594 for iteration= 719\n",
      "loss= 2.614830732345581 for iteration= 720\n",
      "loss= 2.6141884326934814 for iteration= 721\n",
      "loss= 2.6143481731414795 for iteration= 722\n",
      "loss= 2.6137290000915527 for iteration= 723\n",
      "loss= 2.6138694286346436 for iteration= 724\n",
      "loss= 2.6132733821868896 for iteration= 725\n",
      "loss= 2.6133952140808105 for iteration= 726\n",
      "loss= 2.612821578979492 for iteration= 727\n",
      "loss= 2.6129255294799805 for iteration= 728\n",
      "loss= 2.612372875213623 for iteration= 729\n",
      "loss= 2.612459421157837 for iteration= 730\n",
      "loss= 2.6119277477264404 for iteration= 731\n",
      "loss= 2.6119978427886963 for iteration= 732\n",
      "loss= 2.6114861965179443 for iteration= 733\n",
      "loss= 2.611541271209717 for iteration= 734\n",
      "loss= 2.611048698425293 for iteration= 735\n",
      "loss= 2.6110877990722656 for iteration= 736\n",
      "loss= 2.6106135845184326 for iteration= 737\n",
      "loss= 2.6106390953063965 for iteration= 738\n",
      "loss= 2.610183000564575 for iteration= 739\n",
      "loss= 2.6101951599121094 for iteration= 740\n",
      "loss= 2.6097564697265625 for iteration= 741\n",
      "loss= 2.609755039215088 for iteration= 742\n",
      "loss= 2.60933256149292 for iteration= 743\n",
      "loss= 2.609318971633911 for iteration= 744\n",
      "loss= 2.608912706375122 for iteration= 745\n",
      "loss= 2.608887195587158 for iteration= 746\n",
      "loss= 2.6084964275360107 for iteration= 747\n",
      "loss= 2.6084601879119873 for iteration= 748\n",
      "loss= 2.608083486557007 for iteration= 749\n",
      "loss= 2.608037233352661 for iteration= 750\n",
      "loss= 2.6076748371124268 for iteration= 751\n",
      "loss= 2.6076183319091797 for iteration= 752\n",
      "loss= 2.607269048690796 for iteration= 753\n",
      "loss= 2.607203722000122 for iteration= 754\n",
      "loss= 2.6068670749664307 for iteration= 755\n",
      "loss= 2.6067934036254883 for iteration= 756\n",
      "loss= 2.60646915435791 for iteration= 757\n",
      "loss= 2.606387138366699 for iteration= 758\n",
      "loss= 2.606074810028076 for iteration= 759\n",
      "loss= 2.605985403060913 for iteration= 760\n",
      "loss= 2.605684518814087 for iteration= 761\n",
      "loss= 2.605588436126709 for iteration= 762\n",
      "loss= 2.6052980422973633 for iteration= 763\n",
      "loss= 2.6051955223083496 for iteration= 764\n",
      "loss= 2.604914665222168 for iteration= 765\n",
      "loss= 2.6048061847686768 for iteration= 766\n",
      "loss= 2.6045351028442383 for iteration= 767\n",
      "loss= 2.604421377182007 for iteration= 768\n",
      "loss= 2.6041598320007324 for iteration= 769\n",
      "loss= 2.604041337966919 for iteration= 770\n",
      "loss= 2.603787899017334 for iteration= 771\n",
      "loss= 2.6036648750305176 for iteration= 772\n",
      "loss= 2.603419780731201 for iteration= 773\n",
      "loss= 2.6032931804656982 for iteration= 774\n",
      "loss= 2.603055715560913 for iteration= 775\n",
      "loss= 2.6029253005981445 for iteration= 776\n",
      "loss= 2.6026947498321533 for iteration= 777\n",
      "loss= 2.6025614738464355 for iteration= 778\n",
      "loss= 2.602337598800659 for iteration= 779\n",
      "loss= 2.602201461791992 for iteration= 780\n",
      "loss= 2.6019845008850098 for iteration= 781\n",
      "loss= 2.6018457412719727 for iteration= 782\n",
      "loss= 2.601634979248047 for iteration= 783\n",
      "loss= 2.6014938354492188 for iteration= 784\n",
      "loss= 2.6012890338897705 for iteration= 785\n",
      "loss= 2.6011459827423096 for iteration= 786\n",
      "loss= 2.6009464263916016 for iteration= 787\n",
      "loss= 2.600802183151245 for iteration= 788\n",
      "loss= 2.600607395172119 for iteration= 789\n",
      "loss= 2.600461959838867 for iteration= 790\n",
      "loss= 2.6002719402313232 for iteration= 791\n",
      "loss= 2.600125789642334 for iteration= 792\n",
      "loss= 2.5999398231506348 for iteration= 793\n",
      "loss= 2.599792718887329 for iteration= 794\n",
      "loss= 2.5996108055114746 for iteration= 795\n",
      "loss= 2.59946346282959 for iteration= 796\n",
      "loss= 2.59928560256958 for iteration= 797\n",
      "loss= 2.599138021469116 for iteration= 798\n",
      "loss= 2.598963737487793 for iteration= 799\n",
      "loss= 2.598815679550171 for iteration= 800\n",
      "loss= 2.598644971847534 for iteration= 801\n",
      "loss= 2.598496675491333 for iteration= 802\n",
      "loss= 2.5983290672302246 for iteration= 803\n",
      "loss= 2.5981814861297607 for iteration= 804\n",
      "loss= 2.5980162620544434 for iteration= 805\n",
      "loss= 2.5978686809539795 for iteration= 806\n",
      "loss= 2.5977067947387695 for iteration= 807\n",
      "loss= 2.5975594520568848 for iteration= 808\n",
      "loss= 2.5973997116088867 for iteration= 809\n",
      "loss= 2.5972533226013184 for iteration= 810\n",
      "loss= 2.5970959663391113 for iteration= 811\n",
      "loss= 2.596950054168701 for iteration= 812\n",
      "loss= 2.596794605255127 for iteration= 813\n",
      "loss= 2.596649408340454 for iteration= 814\n",
      "loss= 2.596496105194092 for iteration= 815\n",
      "loss= 2.596351385116577 for iteration= 816\n",
      "loss= 2.5962002277374268 for iteration= 817\n",
      "loss= 2.5960559844970703 for iteration= 818\n",
      "loss= 2.5959067344665527 for iteration= 819\n",
      "loss= 2.5957634449005127 for iteration= 820\n",
      "loss= 2.595615863800049 for iteration= 821\n",
      "loss= 2.595473051071167 for iteration= 822\n",
      "loss= 2.595327138900757 for iteration= 823\n",
      "loss= 2.5951855182647705 for iteration= 824\n",
      "loss= 2.5950405597686768 for iteration= 825\n",
      "loss= 2.5948996543884277 for iteration= 826\n",
      "loss= 2.5947561264038086 for iteration= 827\n",
      "loss= 2.594616174697876 for iteration= 828\n",
      "loss= 2.5944743156433105 for iteration= 829\n",
      "loss= 2.594334602355957 for iteration= 830\n",
      "loss= 2.594193696975708 for iteration= 831\n",
      "loss= 2.594055414199829 for iteration= 832\n",
      "loss= 2.5939154624938965 for iteration= 833\n",
      "loss= 2.593777656555176 for iteration= 834\n",
      "loss= 2.593639373779297 for iteration= 835\n",
      "loss= 2.5935022830963135 for iteration= 836\n",
      "loss= 2.593364715576172 for iteration= 837\n",
      "loss= 2.593228340148926 for iteration= 838\n",
      "loss= 2.593092203140259 for iteration= 839\n",
      "loss= 2.592956304550171 for iteration= 840\n",
      "loss= 2.5928211212158203 for iteration= 841\n",
      "loss= 2.592686176300049 for iteration= 842\n",
      "loss= 2.5925512313842773 for iteration= 843\n",
      "loss= 2.5924174785614014 for iteration= 844\n",
      "loss= 2.5922834873199463 for iteration= 845\n",
      "loss= 2.5921502113342285 for iteration= 846\n",
      "loss= 2.5920169353485107 for iteration= 847\n",
      "loss= 2.591884136199951 for iteration= 848\n",
      "loss= 2.591752052307129 for iteration= 849\n",
      "loss= 2.5916199684143066 for iteration= 850\n",
      "loss= 2.5914881229400635 for iteration= 851\n",
      "loss= 2.5913569927215576 for iteration= 852\n",
      "loss= 2.591226100921631 for iteration= 853\n",
      "loss= 2.591095447540283 for iteration= 854\n",
      "loss= 2.5909650325775146 for iteration= 855\n",
      "loss= 2.590834856033325 for iteration= 856\n",
      "loss= 2.590705394744873 for iteration= 857\n",
      "loss= 2.590576410293579 for iteration= 858\n",
      "loss= 2.590446949005127 for iteration= 859\n",
      "loss= 2.590318202972412 for iteration= 860\n",
      "loss= 2.5901896953582764 for iteration= 861\n",
      "loss= 2.5900611877441406 for iteration= 862\n",
      "loss= 2.5899336338043213 for iteration= 863\n",
      "loss= 2.589806318283081 for iteration= 864\n",
      "loss= 2.5896787643432617 for iteration= 865\n",
      "loss= 2.5895519256591797 for iteration= 866\n",
      "loss= 2.5894250869750977 for iteration= 867\n",
      "loss= 2.589298725128174 for iteration= 868\n",
      "loss= 2.589172601699829 for iteration= 869\n",
      "loss= 2.5890464782714844 for iteration= 870\n",
      "loss= 2.588921070098877 for iteration= 871\n",
      "loss= 2.5887959003448486 for iteration= 872\n",
      "loss= 2.5886709690093994 for iteration= 873\n",
      "loss= 2.5885462760925293 for iteration= 874\n",
      "loss= 2.5884218215942383 for iteration= 875\n",
      "loss= 2.5882973670959473 for iteration= 876\n",
      "loss= 2.5881731510162354 for iteration= 877\n",
      "loss= 2.5880494117736816 for iteration= 878\n",
      "loss= 2.587925910949707 for iteration= 879\n",
      "loss= 2.5878028869628906 for iteration= 880\n",
      "loss= 2.587679624557495 for iteration= 881\n",
      "loss= 2.587556838989258 for iteration= 882\n",
      "loss= 2.587434768676758 for iteration= 883\n",
      "loss= 2.5873124599456787 for iteration= 884\n",
      "loss= 2.5871903896331787 for iteration= 885\n",
      "loss= 2.587068796157837 for iteration= 886\n",
      "loss= 2.586946964263916 for iteration= 887\n",
      "loss= 2.5868260860443115 for iteration= 888\n",
      "loss= 2.586704969406128 for iteration= 889\n",
      "loss= 2.5865840911865234 for iteration= 890\n",
      "loss= 2.586463451385498 for iteration= 891\n",
      "loss= 2.586343288421631 for iteration= 892\n",
      "loss= 2.586223602294922 for iteration= 893\n",
      "loss= 2.586103677749634 for iteration= 894\n",
      "loss= 2.5859837532043457 for iteration= 895\n",
      "loss= 2.585864782333374 for iteration= 896\n",
      "loss= 2.5857455730438232 for iteration= 897\n",
      "loss= 2.5856266021728516 for iteration= 898\n",
      "loss= 2.585508346557617 for iteration= 899\n",
      "loss= 2.5853898525238037 for iteration= 900\n",
      "loss= 2.5852715969085693 for iteration= 901\n",
      "loss= 2.585153579711914 for iteration= 902\n",
      "loss= 2.585036277770996 for iteration= 903\n",
      "loss= 2.58491849899292 for iteration= 904\n",
      "loss= 2.584801197052002 for iteration= 905\n",
      "loss= 2.5846846103668213 for iteration= 906\n",
      "loss= 2.5845675468444824 for iteration= 907\n",
      "loss= 2.584451198577881 for iteration= 908\n",
      "loss= 2.5843346118927 for iteration= 909\n",
      "loss= 2.5842185020446777 for iteration= 910\n",
      "loss= 2.5841026306152344 for iteration= 911\n",
      "loss= 2.58398699760437 for iteration= 912\n",
      "loss= 2.583871841430664 for iteration= 913\n",
      "loss= 2.583756446838379 for iteration= 914\n",
      "loss= 2.583641290664673 for iteration= 915\n",
      "loss= 2.583526611328125 for iteration= 916\n",
      "loss= 2.583411931991577 for iteration= 917\n",
      "loss= 2.5832977294921875 for iteration= 918\n",
      "loss= 2.583183526992798 for iteration= 919\n",
      "loss= 2.5830695629119873 for iteration= 920\n",
      "loss= 2.582956314086914 for iteration= 921\n",
      "loss= 2.5828425884246826 for iteration= 922\n",
      "loss= 2.5827293395996094 for iteration= 923\n",
      "loss= 2.5826165676116943 for iteration= 924\n",
      "loss= 2.582503318786621 for iteration= 925\n",
      "loss= 2.582390785217285 for iteration= 926\n",
      "loss= 2.5822787284851074 for iteration= 927\n",
      "loss= 2.5821661949157715 for iteration= 928\n",
      "loss= 2.582054615020752 for iteration= 929\n",
      "loss= 2.5819427967071533 for iteration= 930\n",
      "loss= 2.5818309783935547 for iteration= 931\n",
      "loss= 2.5817196369171143 for iteration= 932\n",
      "loss= 2.581608533859253 for iteration= 933\n",
      "loss= 2.5814976692199707 for iteration= 934\n",
      "loss= 2.5813870429992676 for iteration= 935\n",
      "loss= 2.5812766551971436 for iteration= 936\n",
      "loss= 2.5811660289764404 for iteration= 937\n",
      "loss= 2.5810561180114746 for iteration= 938\n",
      "loss= 2.580946207046509 for iteration= 939\n",
      "loss= 2.580836296081543 for iteration= 940\n",
      "loss= 2.5807271003723145 for iteration= 941\n",
      "loss= 2.580617666244507 for iteration= 942\n",
      "loss= 2.5805084705352783 for iteration= 943\n",
      "loss= 2.580399751663208 for iteration= 944\n",
      "loss= 2.5802910327911377 for iteration= 945\n",
      "loss= 2.5801825523376465 for iteration= 946\n",
      "loss= 2.5800745487213135 for iteration= 947\n",
      "loss= 2.5799663066864014 for iteration= 948\n",
      "loss= 2.5798583030700684 for iteration= 949\n",
      "loss= 2.5797510147094727 for iteration= 950\n",
      "loss= 2.5796432495117188 for iteration= 951\n",
      "loss= 2.579535722732544 for iteration= 952\n",
      "loss= 2.5794289112091064 for iteration= 953\n",
      "loss= 2.579322099685669 for iteration= 954\n",
      "loss= 2.5792152881622314 for iteration= 955\n",
      "loss= 2.579108953475952 for iteration= 956\n",
      "loss= 2.579002618789673 for iteration= 957\n",
      "loss= 2.5788962841033936 for iteration= 958\n",
      "loss= 2.5787906646728516 for iteration= 959\n",
      "loss= 2.5786848068237305 for iteration= 960\n",
      "loss= 2.5785789489746094 for iteration= 961\n",
      "loss= 2.5784738063812256 for iteration= 962\n",
      "loss= 2.5783684253692627 for iteration= 963\n",
      "loss= 2.578263759613037 for iteration= 964\n",
      "loss= 2.5781588554382324 for iteration= 965\n",
      "loss= 2.578054189682007 for iteration= 966\n",
      "loss= 2.5779504776000977 for iteration= 967\n",
      "loss= 2.577845811843872 for iteration= 968\n",
      "loss= 2.5777416229248047 for iteration= 969\n",
      "loss= 2.5776381492614746 for iteration= 970\n",
      "loss= 2.5775344371795654 for iteration= 971\n",
      "loss= 2.5774309635162354 for iteration= 972\n",
      "loss= 2.5773274898529053 for iteration= 973\n",
      "loss= 2.5772244930267334 for iteration= 974\n",
      "loss= 2.5771214962005615 for iteration= 975\n",
      "loss= 2.5770187377929688 for iteration= 976\n",
      "loss= 2.576916456222534 for iteration= 977\n",
      "loss= 2.5768136978149414 for iteration= 978\n",
      "loss= 2.576711654663086 for iteration= 979\n",
      "loss= 2.5766096115112305 for iteration= 980\n",
      "loss= 2.576507806777954 for iteration= 981\n",
      "loss= 2.5764060020446777 for iteration= 982\n",
      "loss= 2.5763044357299805 for iteration= 983\n",
      "loss= 2.5762033462524414 for iteration= 984\n",
      "loss= 2.5761022567749023 for iteration= 985\n",
      "loss= 2.576000928878784 for iteration= 986\n",
      "loss= 2.5759007930755615 for iteration= 987\n",
      "loss= 2.5757999420166016 for iteration= 988\n",
      "loss= 2.5756993293762207 for iteration= 989\n",
      "loss= 2.575599431991577 for iteration= 990\n",
      "loss= 2.5754990577697754 for iteration= 991\n",
      "loss= 2.575399398803711 for iteration= 992\n",
      "loss= 2.5752992630004883 for iteration= 993\n",
      "loss= 2.575199604034424 for iteration= 994\n",
      "loss= 2.5751006603240967 for iteration= 995\n",
      "loss= 2.5750012397766113 for iteration= 996\n",
      "loss= 2.5749025344848633 for iteration= 997\n",
      "loss= 2.574803352355957 for iteration= 998\n",
      "loss= 2.574704647064209 for iteration= 999\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    # forward pass\n",
    "    emb = C[X] #the shape is (32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) #we are using tanh activation fn so the numbers in h will be -1 and 1 \n",
    "    # We pass -1 to emb.view(), becuase we want pytorch to guess what what will be the right number for rows given we have already told it the number of columns are 6\n",
    "    # here you can also use emb.reshape(32,6) the difference is emb.view() will not use extra space it ensure that the emb tensor and and the new tensor that we create will use the same data so no memory wastag\n",
    "    logits = h @ W2 + b2 #the shape is 32,27\n",
    "    # implementing the loss function\n",
    "    loss = F.cross_entropy(logits, Y)  #implementing the categorical cross entropy using pytorch\n",
    "    print(f'loss= {loss.item()} for iteration= {i}') \n",
    "    #we use pytorch here because large positive numbers when exponentiated causes the overflow which can be handled by subtracting the biggest number in the logits from each element row wise, and pytorch handles this operation internally\n",
    "\n",
    "    # Implementing backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # param update\n",
    "    for p in parameters:\n",
    "        p.data -= 0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0858cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in th code above it takes 2 minutes to train the model because we are doing batch gradient descent calculating\n",
    "#loss for every example in the dataset and then updating the weights we can accelarate this by implementing \n",
    "#mini batch SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
