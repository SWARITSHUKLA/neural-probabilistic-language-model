{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876667e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets build the model :)\n"
     ]
    }
   ],
   "source": [
    "print('Lets build the model :)')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#In this modeling approach we are taking 3 previous and trying to predict the 4th word in the sequance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be210e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "141884cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28dc6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the mappings of characters to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff63d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the dataset\n",
    "block_size = 3 #context length how many characters do we take to predict the next one\n",
    "X,Y = [],[]\n",
    "for w in words[:5]:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] #crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "483c71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lookup table for the character embeddings\n",
    "C = torch.randn((27,2))\n",
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d5e53ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2458a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructiong the hidden layer\n",
    "#The number of inputs to this layer is going to be 3 * 2  because we hae to dimensional embeddings and we have 3 of them\n",
    "# and its up to us to decide how many neurons we want inside the layer here we are going with 100 of them\n",
    "W1 = torch.randn(6,100)\n",
    "b1 = torch.tensor(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f421ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.view(32,6)\n",
    "# here you can also use emb.reshape(32,6) the difference is emb.view() will not use extra space it ensure that the emb tensor and and the new tensor that we create will use the same data so no memory wastage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e5c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb,1),1) == emb.view(32,6)\n",
    "#here torch.unbind unbinds the tensor into 3 parts because we have passed 1 to it and the shape of current tensor is 32,3,2 and the indexing 1 corresponds to 3\n",
    "# and then torch.cat() concatinates the 3 vector and we pass it 1 because we want to make it concatinate along axis 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
